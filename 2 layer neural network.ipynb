{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyM5kIgo0Xd45dfEPxWouiOp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pbMoPgB9fQIr"},"source":["Simplistic implementation of the two-layer neural network.\n","Training method is stochastic (online) gradient descent with momentum.\n","As an example it computes XOR for given input.\n","Some details:\n","- tanh activation for hidden layer\n","- sigmoid activation for output layer\n","- cross-entropy loss"]},{"cell_type":"code","metadata":{"id":"cbMOlRGffU7E","executionInfo":{"status":"ok","timestamp":1620385359209,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}}},"source":["import numpy as np\n","import time\n","\n","n_hidden = 10\n","n_in = 10\n","n_out = 10\n","n_samples = 300\n","\n","learning_rate = 0.01\n","momentum = 0.9\n","\n","np.random.seed(0)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnemEMUbfq2L","executionInfo":{"status":"ok","timestamp":1620385443399,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}}},"source":["def sigmoid(x):\n","    return 1.0/(1.0 + np.exp(-x))\n","\n","def tanh_prime(x):\n","    return  1 - np.tanh(x)**2\n","\n","def train(x, t, V, W, bv, bw):\n","\n","    # forward\n","    A = np.dot(x, V) + bv\n","    Z = np.tanh(A)\n","\n","    B = np.dot(Z, W) + bw\n","    Y = sigmoid(B)\n","\n","    # backward\n","    Ew = Y - t\n","    Ev = tanh_prime(A) * np.dot(W, Ew)\n","\n","    dW = np.outer(Z, Ew)\n","    dV = np.outer(x, Ev)\n","\n","    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n","\n","    # Note that we use error for each layer as a gradient\n","    # for biases\n","\n","    return  loss, (dV, dW, Ev, Ew)\n","\n","def predict(x, V, W, bv, bw):\n","    A = np.dot(x, V) + bv\n","    B = np.dot(np.tanh(A), W) + bw\n","    return (sigmoid(B) > 0.5).astype(int)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmfqofXqf_aD","executionInfo":{"status":"ok","timestamp":1620385481508,"user_tz":-330,"elapsed":1235,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}}},"source":["# Setup initial parameters\n","# Note that initialization is cruxial for first-order methods!\n","\n","V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n","W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n","\n","bv = np.zeros(n_hidden)\n","bw = np.zeros(n_out)\n","\n","params = [V,W,bv,bw]\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXngQwKWgHwa","executionInfo":{"status":"ok","timestamp":1620385519138,"user_tz":-330,"elapsed":1076,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}}},"source":["# Generate some data\n","\n","X = np.random.binomial(1, 0.5, (n_samples, n_in))\n","T = X ^ 1\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"id":"ZjGN6T_OgQll","executionInfo":{"status":"error","timestamp":1620385770943,"user_tz":-330,"elapsed":1036,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}},"outputId":"7daca29f-8eba-4be4-e296-21706c0fa979"},"source":["# Train\n","for epoch in range(100):\n","    err = []\n","    upd = [0]*len(params)\n","\n","    t0 = time.clock()\n","    for i in range(X.shape[0]):\n","        loss, grad = train(X[i], T[i], *params)\n","\n","        for j in range(len(params)):\n","            params[j] -= upd[j]\n","\n","        for j in range(len(params)):\n","            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n","\n","        err.append( loss )\n","\n","    print \"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (\n","                epoch, np.mean( err ), time.clock()-t0 )"],"execution_count":10,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-6c6e85331a4a>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print \"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"id":"9En3vuTfghpU","executionInfo":{"status":"error","timestamp":1620385678783,"user_tz":-330,"elapsed":1214,"user":{"displayName":"Udara Vimukthi Lakshan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh27bhpBx9Zqb0AUKJ9kVUkZEeoZqiLbVJpw8CQkA=s64","userId":"05896419729734565722"}},"outputId":"59cef969-e328-48a7-92a1-299aa098d283"},"source":["# Try to predict something\n","\n","x = np.random.binomial(1, 0.5, n_in)\n","print \"XOR prediction:\"\n","print x\n","print predict(x, *params)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-381ca2c46953>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print \"XOR prediction:\"\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"XOR prediction:\")?\n"]}]}]}